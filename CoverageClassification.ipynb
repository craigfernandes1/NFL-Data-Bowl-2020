{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% Import Packages\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import nflFunctions as f\n",
    "\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import data\n",
    "games_df = pd.read_csv(Path.cwd() / 'data' /'games.csv')\n",
    "players_df = pd.read_csv(Path.cwd() / 'data' /'players.csv')\n",
    "plays_df = pd.read_csv(Path.cwd() / 'data' /'plays.csv')\n",
    "tracking_df = pd.read_csv(Path.cwd() / 'data' /'week1.csv')\n",
    "coverage_df = pd.read_csv(Path.cwd() / 'data' /'coverages_week1.csv')\n",
    "\n",
    "# Create coverage mapping dict\n",
    "mapping_dict = {\"Cover 0 Man\": 0, \"Cover 1 Man\": 1,  # Boolean replacements for no/yes\n",
    "                \"Cover 2 Man\":2, \"Cover 2 Zone\":3,\n",
    "                \"Cover 3 Zone\":4, \"Cover 4 Zone\":5,\n",
    "                \"Cover 6 Zone\":6, \"Prevent Zone\":7}  # Boolean replacements for abnormal/normal\n",
    "\n",
    "# Just Zone or Man\n",
    "# mapping_dict = {\"Cover 0 Man\": 0, \"Cover 1 Man\": 0,  # Boolean replacements for no/yes\n",
    "#                 \"Cover 2 Man\":0, \"Cover 2 Zone\":1,\n",
    "#                 \"Cover 3 Zone\":1, \"Cover 4 Zone\":1,\n",
    "#                 \"Cover 6 Zone\":1, \"Prevent Zone\":1}  #\n",
    "\n",
    "# Apply the mapping dict to the data\n",
    "coverage_df.replace(mapping_dict, inplace=True)\n",
    "\n",
    "# Merge coverage, games and plays df\n",
    "total_df = pd.merge(pd.merge(coverage_df, games_df, on=['gameId']), plays_df, on=['playId','gameId'])\n",
    "total_df['scrimmageLine'] = total_df.absoluteYardlineNumber\n",
    "total_df = total_df[total_df.playType != 'play_type_unknown']\n",
    "\n",
    "# Important vectors to use\n",
    "np.random.seed(0)\n",
    "defList = ['CB','DB','DE','DT','FS', 'ILB', 'LB','LS','MLB','NT','OLB', 'S', 'SS']\n",
    "xBin = [0,2,4,6,8,10,12,14,16,18,20,25,30,50,100]\n",
    "yBin = [0,8,14,20,26,32,38,44,53]\n",
    "# labels = ['tight', 'close', 'medium', 'far', 'deep', 'extreme_deep']\n",
    "\n",
    "# Create train/test split\n",
    "split = np.random.rand(len(total_df)) < 0.7\n",
    "train_df = total_df[split]\n",
    "test_df = total_df[~split]\n",
    "\n",
    "# Get tracking data\n",
    "tracking_train = pd.merge(tracking_df, train_df[['gameId','playId']])\n",
    "tracking_test = pd.merge(tracking_df, test_df[['gameId','playId']])\n",
    "\n",
    "# train data preprocessing\n",
    "total_train_df = pd.merge(train_df, tracking_train, on=['playId','gameId'])\n",
    "total_train_df.drop(columns=['playDescription'], inplace=True)\n",
    "total_train_df = total_train_df[(total_train_df.event == 'ball_snap') & (total_train_df.position.isin(defList))]\n",
    "total_train_df['absXdiff'] = abs(total_train_df.scrimmageLine - total_train_df.x)\n",
    "total_train_df['defenderY'] = (pd.cut(total_train_df.y, yBin, labels=False, retbins=True, right=False))[0]\n",
    "total_train_df = pd.get_dummies(total_train_df, columns=['defenderY']) \n",
    "total_train_df['defenderX'] = (pd.cut(total_train_df.absXdiff, xBin, labels=False, retbins=True, right=False))[0]\n",
    "total_train_df = pd.get_dummies(total_train_df, columns=['defenderX']) \n",
    "\n",
    "# test data preprocessing\n",
    "total_test_df = pd.merge(test_df, tracking_test, on=['playId','gameId'])\n",
    "total_test_df.drop(columns=['playDescription'], inplace=True)\n",
    "total_test_df = total_test_df[(total_test_df.event == 'ball_snap') & (total_test_df.position.isin(defList))]\n",
    "total_test_df['absXdiff'] = abs(total_test_df.scrimmageLine - total_test_df.x)\n",
    "total_test_df['defenderY'] = (pd.cut(total_test_df.y, yBin, labels=False, retbins=True, right=False))[0]\n",
    "total_test_df = pd.get_dummies(total_test_df, columns=['defenderY']) \n",
    "total_test_df['defenderX'] = (pd.cut(total_test_df.absXdiff, xBin, labels=False, retbins=True, right=False))[0]\n",
    "total_test_df = pd.get_dummies(total_test_df, columns=['defenderX']) \n",
    "\n",
    "\n",
    "# Delete the smaller dfs\n",
    "df_list = [games_df, plays_df, tracking_df, coverage_df, total_df, tracking_test, split, tracking_train, ]\n",
    "del games_df, plays_df, tracking_df, coverage_df, total_df, tracking_test, split,tracking_train, \n",
    "del df_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Import data and create training/testing df\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "cols = ['x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','y0','y1','y2','y3','y4','y5','y6','y7']\n",
    "\n",
    "# Get training defender counts\n",
    "summary_train = total_train_df.groupby(['gameId', 'playId']).sum()\n",
    "summary_train = summary_train.iloc[:,27:49]\n",
    "summary_train.columns = cols\n",
    "train_df = pd.merge(train_df, summary_train, on=['gameId','playId'])\n",
    "\n",
    "# Get testing defender counts\n",
    "summary_test = total_test_df.groupby(['gameId', 'playId']).sum()\n",
    "summary_test = summary_test.iloc[:,27:49]\n",
    "summary_test['defenderX_13'] = np.zeros(summary_test.shape[0])\n",
    "summary_test.columns = cols\n",
    "test_df = pd.merge(test_df, summary_test, on=['gameId','playId'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get summary counts of each defender depth\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# train_df['pointdiff'] = abs(train_df.preSnapHomeScore-train_df.preSnapVisitorScore)\n",
    "train_df = pd.get_dummies(train_df,columns=['personnelD'])\n",
    "# test_df['pointdiff'] = abs(test_df.preSnapHomeScore-test_df.preSnapVisitorScore)\n",
    "test_df = pd.get_dummies(test_df,columns=['personnelD'])\n",
    "\n",
    "# fill in missing cols for test_df\n",
    "missing_dummy_columns = train_df.columns[55:].difference(test_df.columns[55:])\n",
    "for i in range(missing_dummy_columns.shape[0]):\n",
    "    test_df[missing_dummy_columns[i]] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Make derived features for point diff and offensive formation\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:,33:]\n",
    "y_train = train_df.iloc[:,2]\n",
    "X_test = test_df.iloc[:,33:]\n",
    "y_test = test_df.iloc[:,2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create X_train, y_train\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "X_train.drop(columns=['y0','y1','y2','y3','y4','y5','y6','y7'], inplace=True)\n",
    "X_test.drop(columns=['y0','y1','y2','y3','y4','y5','y6','y7'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Import sklearn functions\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the accuracy on the: \n",
      "\t training data is 0.816\n",
      "\t testing data is 0.35\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "cart_model = DecisionTreeClassifier(max_depth=10)\n",
    "train_score, test_score = f.fit_and_score_model(cart_model, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% CART\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the accuracy on the: \n",
      "\t training data is 0.726\n",
      "\t testing data is 0.465\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators = 250, max_samples = 0.2)\n",
    "train_score, test_score = f.fit_and_score_model(rf_model, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% RF\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the accuracy on the: \n",
      "\t training data is 0.52\n",
      "\t testing data is 0.493\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "logregcv = LogisticRegression(max_iter=9000, class_weight=None, solver='liblinear', penalty='l2')\n",
    "train_score, test_score = f.fit_and_score_model(logregcv, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% LogRegCV\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the accuracy on the: \n",
      "\t training data is 0.327\n",
      "\t testing data is 0.315\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=250, learning_rate=0.28)\n",
    "train_score, test_score = f.fit_and_score_model(ada, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Adaboosting\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the accuracy on the: \n",
      "\t training data is 0.971\n",
      "\t testing data is 0.374\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "grad = GradientBoostingClassifier(n_estimators=100, learning_rate=0.30, criterion='mse')\n",
    "train_score, test_score = f.fit_and_score_model(grad, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Gradientboosting\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the accuracy on the: \n",
      "\t training data is 0.53\n",
      "\t testing data is 0.448\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# mdl = svm.SVC(decision_function_shape='ovo')\n",
    "mdl = LinearSVC(max_iter=10000, tol=1e-8)\n",
    "train_score, test_score = f.fit_and_score_model(mdl, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% SVM\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# bayes = CategoricalNB()\n",
    "# train_score, test_score = f.fit_and_score_model(bayes, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Naive Bayes\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "No handles with labels found to put in legend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# gid = 2018090901\n",
    "# pid = 2055\n",
    "# \n",
    "# # Just for test purposes\n",
    "# play = total_train_df[(total_train_df.gameId == gid) & (total_train_df.playId == pid)]\n",
    "# fig, ax = f.create_football_field(highlight_line=True,  highlight_line_number=play.scrimmageLine.values[0])                                  \n",
    "# play.plot(x='x', y='y', kind='scatter', ax=ax, color='blue', s=30, legend='Defense')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Plot one Play\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}